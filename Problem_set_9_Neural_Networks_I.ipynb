{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2A9WlOg2Ymzy"
   },
   "source": [
    "# Exersise 1:\n",
    "Play with the code you have worked out in the lecture. For example, change the input data and the output data and try to get a good understanding of what is happening and how the neural network adjusts the weights. Then, create a Neural Network with a weight matrix of dimension (6,1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the code ran in the class for the Neural Network with a weight matrix of dimension of 5 x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VV3JHLpwYjE4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data\n",
      "Input row 0: [0 0 1 0 0]\n",
      "Input row 1: [1 1 0 1 1]\n",
      "Input row 2: [1 0 0 1 0]\n",
      "Input row 3: [0 1 1 1 0]\n",
      "\n",
      "Target data\n",
      "Target neuron 0: [0]\n",
      "Target neuron 1: [1]\n",
      "Target neuron 2: [1]\n",
      "Target neuron 3: [0]\n",
      "\n",
      "Hidden layer weights\n",
      "Weight of input-column 0: [4.26399604]\n",
      "Weight of input-column 1: [-0.79907371]\n",
      "Weight of input-column 2: [-5.43402607]\n",
      "Weight of input-column 3: [1.02167938]\n",
      "Weight of input-column 4: [1.06672687]\n",
      "\n",
      "Predicted Output After Training\n",
      "Predicted output 0: [0.00434672]\n",
      "Predicted output 1: [0.99614022]\n",
      "Predicted output 2: [0.99496164]\n",
      "Predicted output 3: [0.00542462]\n"
     ]
    }
   ],
   "source": [
    "#CLASS code 5 x 1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Input data (X) and target outputs (y)\n",
    "X = np.array([[0, 0, 1, 0, 0],\n",
    "              [1, 1, 0, 1, 1],\n",
    "              [1, 0, 0, 1, 0],\n",
    "              [0, 1, 1, 1, 0]])\n",
    "y = np.array([[0, 1, 1, 0]]).T\n",
    "\n",
    "# Activation function (sigmoid) and its derivative\n",
    "def nonlin(x, deriv=False):\n",
    "    if deriv:\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Initialize random weights for a single-layer neural network\n",
    "np.random.seed(1)\n",
    "weights = 2 * np.random.random((5, 1)) - 1\n",
    "\n",
    "# Training loop\n",
    "for iter in range(10000):\n",
    "    # Forward propagation\n",
    "    l0 = X  # Input layer\n",
    "    l1 = nonlin(np.dot(l0, weights))  # Predicted output (layer 1)\n",
    "    \n",
    "    # Calculate the error\n",
    "    l1_error = y - l1\n",
    "    \n",
    "    # Calculate the delta (error weighted by sigmoid derivative)\n",
    "    l1_delta = l1_error * nonlin(l1, True)\n",
    "    \n",
    "    # Update weights based on delta\n",
    "    weights += np.dot(l0.T, l1_delta)\n",
    "\n",
    "# Print results\n",
    "print(\"Input data\")\n",
    "print(*('Input row {}: {}'.format(*k) for k in enumerate(l0)), sep=\"\\n\")\n",
    "print(\"\\nTarget data\")\n",
    "print(*('Target neuron {}: {}'.format(*k) for k in enumerate(y)), sep=\"\\n\")\n",
    "print(\"\\nHidden layer weights\")\n",
    "print(*('Weight of input-column {}: {}'.format(*k) for k in enumerate(weights)), sep=\"\\n\")\n",
    "print(\"\\nPredicted Output After Training\")\n",
    "print(*('Predicted output {}: {}'.format(*k) for k in enumerate(l1)), sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is the code of the adjusted input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified Input Data and Target Outputs\n",
      "\n",
      "Input data\n",
      "Input row 0: [0 1 1 0 0]\n",
      "Input row 1: [1 0 0 1 1]\n",
      "Input row 2: [1 1 0 0 1]\n",
      "Input row 3: [0 0 1 1 0]\n",
      "\n",
      "Target data\n",
      "Target neuron 0: [1]\n",
      "Target neuron 1: [0]\n",
      "Target neuron 2: [1]\n",
      "Target neuron 3: [0]\n",
      "\n",
      "Hidden layer weights\n",
      "Weight of input-column 0: [0.0586548]\n",
      "Weight of input-column 1: [5.70652461]\n",
      "Weight of input-column 2: [-0.42324047]\n",
      "Weight of input-column 3: [-4.86006891]\n",
      "Weight of input-column 4: [-0.48187743]\n",
      "\n",
      "Predicted Output After Training\n",
      "Predicted output 0: [0.99494964]\n",
      "Predicted output 1: [0.00505032]\n",
      "Predicted output 2: [0.99494973]\n",
      "Predicted output 3: [0.00505023]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Different input data\n",
    "X = np.array([[0, 1, 1, 0, 0],  \n",
    "              [1, 0, 0, 1, 1],\n",
    "              [1, 1, 0, 0, 1],\n",
    "              [0, 0, 1, 1, 0]])\n",
    "\n",
    "# Corresponding output data\n",
    "y = np.array([[1, 0, 1, 0]]).T  \n",
    "\n",
    "# Activation function\n",
    "def nonlin(x, deriv=False):\n",
    "    if deriv:\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Initialize weights\n",
    "np.random.seed(1)\n",
    "weights = 2 * np.random.random((5, 1)) - 1\n",
    "\n",
    "# Training loop\n",
    "for iter in range(10000):\n",
    "    # Forward pass\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0, weights))\n",
    "    \n",
    "    # Error and adjustment\n",
    "    l1_error = y - l1\n",
    "    l1_delta = l1_error * nonlin(l1, True)\n",
    "    \n",
    "    # Update weights\n",
    "    weights += np.dot(l0.T, l1_delta)\n",
    "\n",
    "# Results\n",
    "print(\"Modified Input Data and Target Outputs\")\n",
    "print(\"\\nInput data\")\n",
    "print(*('Input row {}: {}'.format(*k) for k in enumerate(l0)), sep=\"\\n\")\n",
    "print(\"\\nTarget data\")\n",
    "print(*('Target neuron {}: {}'.format(*k) for k in enumerate(y)), sep=\"\\n\")\n",
    "print(\"\\nHidden layer weights\")\n",
    "print(*('Weight of input-column {}: {}'.format(*k) for k in enumerate(weights)), sep=\"\\n\")\n",
    "print(\"\\nPredicted Output After Training\")\n",
    "print(*('Predicted output {}: {}'.format(*k) for k in enumerate(l1)), sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "The neural network successfully adjusted its weights to align predictions with the modified target values. The weights changed to emphasize features that were more correlated with the updated target variable ùë¶.\n",
    "\n",
    "Predicted Outputs after training is close to the real output demonstrating that the network learned the new mapping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing only the target variable of the code ran in class (Cell 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified Target Outputs Only\n",
      "\n",
      "Input data\n",
      "Input row 0: [0 0 1 0 0]\n",
      "Input row 1: [1 1 0 1 1]\n",
      "Input row 2: [1 0 0 1 0]\n",
      "Input row 3: [0 1 1 1 0]\n",
      "\n",
      "New Target data\n",
      "Target neuron 0: [0]\n",
      "Target neuron 1: [1]\n",
      "Target neuron 2: [0]\n",
      "Target neuron 3: [1]\n",
      "\n",
      "Hidden layer weights\n",
      "Weight of input-column 0: [-5.21556039]\n",
      "Weight of input-column 1: [8.69531265]\n",
      "Weight of input-column 2: [-4.46612929]\n",
      "Weight of input-column 3: [0.49867599]\n",
      "Weight of input-column 4: [1.6045602]\n",
      "\n",
      "Predicted Output After Training\n",
      "Predicted output 0: [0.01136173]\n",
      "Predicted output 1: [0.99625259]\n",
      "Predicted output 2: [0.00886418]\n",
      "Predicted output 3: [0.99123173]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input data (same as cell 1)\n",
    "X = np.array([[0, 0, 1, 0, 0],\n",
    "              [1, 1, 0, 1, 1],\n",
    "              [1, 0, 0, 1, 0],\n",
    "              [0, 1, 1, 1, 0]])\n",
    "\n",
    "# New target output (y) \n",
    "y = np.array([[0, 1, 0, 1]]).T  \n",
    "\n",
    "# Activation function and its derivative\n",
    "def nonlin(x, deriv=False):\n",
    "    if deriv:\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Initialize weights\n",
    "np.random.seed(1)\n",
    "weights = 2 * np.random.random((5, 1)) - 1 \n",
    "\n",
    "# Training loop\n",
    "for iter in range(10000):\n",
    "    # Forward pass\n",
    "    l0 = X  # Input layer\n",
    "    l1 = nonlin(np.dot(l0, weights))  # Predicted output\n",
    "    \n",
    "    # Error and adjustment\n",
    "    l1_error = y - l1\n",
    "    l1_delta = l1_error * nonlin(l1, True)\n",
    "    \n",
    "    # Update weights\n",
    "    weights += np.dot(l0.T, l1_delta)\n",
    "\n",
    "# Results\n",
    "print(\"Modified Target Outputs Only\")\n",
    "print(\"\\nInput data\")\n",
    "print(*('Input row {}: {}'.format(*k) for k in enumerate(l0)), sep=\"\\n\")\n",
    "print(\"\\nNew Target data\")\n",
    "print(*('Target neuron {}: {}'.format(*k) for k in enumerate(y)), sep=\"\\n\")\n",
    "print(\"\\nHidden layer weights\")\n",
    "print(*('Weight of input-column {}: {}'.format(*k) for k in enumerate(weights)), sep=\"\\n\")\n",
    "print(\"\\nPredicted Output After Training\")\n",
    "print(*('Predicted output {}: {}'.format(*k) for k in enumerate(l1)), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "The network adjusted weights to minimize the error between the unchanged input data and the new target outputs. Features in \n",
    "input variable with stronger correlations to the new target variablr gained higher weights.\n",
    "\n",
    "The predicted output aligned closely with the new target variable, showing that the network can adapt to different targets while keeping the input static."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Neural Network with a weight matrix of dimension (6,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add More Input Features\n",
      "\n",
      "Input data\n",
      "Input row 0: [0 0 1 0 0 1]\n",
      "Input row 1: [1 1 0 1 1 0]\n",
      "Input row 2: [1 0 0 1 0 1]\n",
      "Input row 3: [0 1 1 1 0 0]\n",
      "\n",
      "Target data\n",
      "Target neuron 0: [0]\n",
      "Target neuron 1: [1]\n",
      "Target neuron 2: [1]\n",
      "Target neuron 3: [0]\n",
      "\n",
      "Hidden layer weights\n",
      "Weight of input-column 0: [4.17557814]\n",
      "Weight of input-column 1: [-0.71459809]\n",
      "Weight of input-column 2: [-5.51042649]\n",
      "Weight of input-column 3: [0.9803944]\n",
      "Weight of input-column 4: [1.10406958]\n",
      "Weight of input-column 5: [0.17080315]\n",
      "\n",
      "Predicted Output After Training\n",
      "Predicted output 0: [0.004775]\n",
      "Predicted output 1: [0.99610978]\n",
      "Predicted output 2: [0.99516355]\n",
      "Predicted output 3: [0.00524835]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input data with 6 features (columns) and 4 samples (rows). I editted the input of the written class code (cell 1)\n",
    "X = np.array([[0, 0, 1, 0, 0, 1],  \n",
    "              [1, 1, 0, 1, 1, 0],\n",
    "              [1, 0, 0, 1, 0, 1],\n",
    "              [0, 1, 1, 1, 0, 0]])\n",
    "\n",
    "# Target outputs (same output as the one written in the class)\n",
    "y = np.array([[0, 1, 1, 0]]).T\n",
    "\n",
    "# Activation function and its derivative\n",
    "def nonlin(x, deriv=False):\n",
    "    if deriv:\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Initialize weights for 6 input features\n",
    "np.random.seed(1)\n",
    "weights = 2 * np.random.random((6, 1)) - 1  # Now 6 weights (one for each feature)\n",
    "\n",
    "# Training loop\n",
    "for iter in range(10000):\n",
    "    # Forward pass\n",
    "    l0 = X  # Input layer\n",
    "    l1 = nonlin(np.dot(l0, weights))  # Predicted output (layer 1)\n",
    "    \n",
    "    # Error and adjustment\n",
    "    l1_error = y - l1\n",
    "    l1_delta = l1_error * nonlin(l1, True)\n",
    "    \n",
    "    # Update weights\n",
    "    weights += np.dot(l0.T, l1_delta)\n",
    "\n",
    "# Results\n",
    "print(\"Add More Input Features\")\n",
    "print(\"\\nInput data\")\n",
    "print(*('Input row {}: {}'.format(*k) for k in enumerate(l0)), sep=\"\\n\")\n",
    "print(\"\\nTarget data\")\n",
    "print(*('Target neuron {}: {}'.format(*k) for k in enumerate(y)), sep=\"\\n\")\n",
    "print(\"\\nHidden layer weights\")\n",
    "print(*('Weight of input-column {}: {}'.format(*k) for k in enumerate(weights)), sep=\"\\n\")\n",
    "print(\"\\nPredicted Output After Training\")\n",
    "print(*('Predicted output {}: {}'.format(*k) for k in enumerate(l1)), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "The additional features input variable introduced new patterns for the network to learn.\n",
    "The network updated its weight matrix, highlighting which new features were most influential. The predicted outputs indicates successful learning despite the increased feature set.\n",
    "\n",
    "Adding features increases the network‚Äôs capacity to learn more complex patterns but requires more iterations for convergence.\n",
    "\n",
    "Weights dynamically adapt to emphasize features that contribute more to minimizing the error between predictions and targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-RwhWW3ZdX5"
   },
   "source": [
    "# Exersise 2:\n",
    "What happens if you reduce the number of iterations of your Forward Propagation algorithm?\n",
    "\n",
    "Demonstrating using code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dAf2eXIuZrk9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results after 100 iterations:\n",
      "\n",
      "Predicted Output After Training\n",
      "Predicted output 0: [0.05114492]\n",
      "Predicted output 1: [0.95897072]\n",
      "Predicted output 2: [0.94502539]\n",
      "Predicted output 3: [0.05810223]\n",
      "\n",
      "Final Hidden Layer Weights\n",
      "Weight of input-column 0: [1.81859135]\n",
      "Weight of input-column 1: [-0.89688446]\n",
      "Weight of input-column 2: [-2.92625473]\n",
      "Weight of input-column 3: [1.03150998]\n",
      "Weight of input-column 4: [1.20416771]\n",
      "\n",
      "Results after 500 iterations:\n",
      "\n",
      "Predicted Output After Training\n",
      "Predicted output 0: [0.01675624]\n",
      "Predicted output 1: [0.97992696]\n",
      "Predicted output 2: [0.97830285]\n",
      "Predicted output 3: [0.02665299]\n",
      "\n",
      "Final Hidden Layer Weights\n",
      "Weight of input-column 0: [2.73953086]\n",
      "Weight of input-column 1: [-0.59601284]\n",
      "Weight of input-column 2: [-4.07305357]\n",
      "Weight of input-column 3: [1.07012677]\n",
      "Weight of input-column 4: [0.67557328]\n",
      "\n",
      "Results after 1000 iterations:\n",
      "\n",
      "Predicted Output After Training\n",
      "Predicted output 0: [0.01666229]\n",
      "Predicted output 1: [0.98810974]\n",
      "Predicted output 2: [0.9842473]\n",
      "Predicted output 3: [0.01617774]\n",
      "\n",
      "Final Hidden Layer Weights\n",
      "Weight of input-column 0: [3.5624794]\n",
      "Weight of input-column 1: [-0.60289209]\n",
      "Weight of input-column 2: [-4.07833521]\n",
      "Weight of input-column 3: [0.57289671]\n",
      "Weight of input-column 4: [0.88812282]\n",
      "\n",
      "Results after 10000 iterations:\n",
      "\n",
      "Predicted Output After Training\n",
      "Predicted output 0: [0.00497373]\n",
      "Predicted output 1: [0.99660898]\n",
      "Predicted output 2: [0.99478097]\n",
      "Predicted output 3: [0.00505608]\n",
      "\n",
      "Final Hidden Layer Weights\n",
      "Weight of input-column 0: [4.13784641]\n",
      "Weight of input-column 1: [-1.0959132]\n",
      "Weight of input-column 2: [-5.29864829]\n",
      "Weight of input-column 3: [1.1124163]\n",
      "Weight of input-column 4: [1.52892815]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input data (X) and target outputs (y)\n",
    "X = np.array([[0, 0, 1, 0, 0],\n",
    "              [1, 1, 0, 1, 1],\n",
    "              [1, 0, 0, 1, 0],\n",
    "              [0, 1, 1, 1, 0]])\n",
    "y = np.array([[0, 1, 1, 0]]).T\n",
    "\n",
    "# Activation function and its derivative\n",
    "def nonlin(x, deriv=False):\n",
    "    if deriv:\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Initialize weights\n",
    "np.random.seed(1)\n",
    "weights = 2 * np.random.random((5, 1)) - 1\n",
    "\n",
    "# Test with different iteration counts\n",
    "iterations_to_test = [100, 500, 1000, 10000]\n",
    "\n",
    "for iters in iterations_to_test:\n",
    "    # Reset weights for each test\n",
    "    weights = 2 * np.random.random((5, 1)) - 1\n",
    "    \n",
    "    # Training loop\n",
    "    for iter in range(iters):\n",
    "        # Forward pass\n",
    "        l0 = X\n",
    "        l1 = nonlin(np.dot(l0, weights))\n",
    "        \n",
    "        # Error and adjustment\n",
    "        l1_error = y - l1\n",
    "        l1_delta = l1_error * nonlin(l1, True)\n",
    "        \n",
    "        # Update weights\n",
    "        weights += np.dot(l0.T, l1_delta)\n",
    "    \n",
    "    # Print results for this iteration count\n",
    "    print(f\"\\nResults after {iters} iterations:\")\n",
    "    print(\"\\nPredicted Output After Training\")\n",
    "    print(*('Predicted output {}: {}'.format(*k) for k in enumerate(l1)), sep=\"\\n\")\n",
    "    print(\"\\nFinal Hidden Layer Weights\")\n",
    "    print(*('Weight of input-column {}: {}'.format(*k) for k in enumerate(weights)), sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "Reducing the number of iterations in the neural network training results in less accurate predictions and incomplete learning. With fewer iterations, the model doesn't have enough time to adjust the weights fully, leading to higher error in the predicted outputs. As the number of iterations increases, the model gradually learns and fine-tunes the weights, producing more accurate predictions. After 10,000 iterations, the network converges, yielding very low error and optimized weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ImwfjN3ZsHm"
   },
   "source": [
    "# Exersise 3:\n",
    "Below is a function written in Python. The function takes an input (x) and transforms it into an output (y). Can you explain what kind of transformation it does? Can you think of why it might be useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jXA5MN5BdES-",
    "outputId": "5a3e5a37-b2e4-444b-f210-a2373dbfe371"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01521943 0.0413707  0.11245721 0.83095266]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\" applies softmax to an input x\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "x = np.array([1, 2, 3, 5])\n",
    "y = softmax(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Observation\n",
    "\n",
    "The function softmax(x) transforms a vector of real values into a probability distribution by exponentiating each element and then normalizing the result so that the sum of the outputs equals 1. This ensures that each output is between 0 and 1, representing the relative probabilities of each class. It is commonly used in multiclass classification problems to interpret the output of a model as probabilities, helping to determine the most likely class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
